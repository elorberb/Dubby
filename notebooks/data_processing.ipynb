{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-17T11:12:07.565530800Z",
     "start_time": "2024-02-17T11:12:07.206078800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/utterances.csv')  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T11:13:14.763839Z",
     "start_time": "2024-02-17T11:13:10.876964600Z"
    }
   },
   "id": "1f33929d3f3ce6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will initiate data processing by primarily saving only the text and labels of the comments. For the preliminary training phase, our strategy is to deploy elementary models that utilize solely the text to evaluate their performance. Consequently, we will extract just the text and label fields and partition them into separate datasets for training, testing, and validation purposes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb0d33d67c64f192"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(15771, 1971, 1972)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 & 2: Filter the DataFrame to include only the necessary columns ('text' and 'meta.success') and clean the text if necessary\n",
    "# For simplicity, we're directly using the 'text' column; additional cleaning might be needed depending on the model requirements\n",
    "data = df[['text', 'meta.success']].dropna(subset=['meta.success'])\n",
    "data['label'] = data['meta.success'].apply(lambda x: 1 if x == 1.0 else 0)\n",
    "\n",
    "# Step 3: Split the dataset into training, validation, and ttype(train, temp = train_test_split(data, test_size=0.2, random_stype(train, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Display the sizes of each dataset to confirm successful split\n",
    "len(train), len(validation), len(test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T11:23:02.947023400Z",
     "start_time": "2024-02-17T11:23:02.871204900Z"
    }
   },
   "id": "e7e0fa3b47ac59d3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "TRAINING SET:\n",
      "Label 1: 0.6283685245070065 instances\n",
      "Label 0: 0.37163147549299347 instances\n",
      "\n",
      "VALIDATION SET:\n",
      "Label 1: 0.639269406392694 instances\n",
      "Label 0: 0.3607305936073059 instances\n",
      "\n",
      "TEST SET:\n",
      "Label 1: 0.6338742393509128 instances\n",
      "Label 0: 0.3661257606490872 instances\n"
     ]
    }
   ],
   "source": [
    "# Calculate and display the distribution of the label for each subset of the data\n",
    "label_distribution_train = train['label'].value_counts(normalize=True)\n",
    "label_distribution_validation = validation['label'].value_counts(normalize=True)\n",
    "label_distribution_test = test['label'].value_counts(normalize=True)\n",
    "\n",
    "  \n",
    "print(\"Label Distribution:\")  \n",
    "print(\"TRAINING SET:\")  \n",
    "for label, count in label_distribution_train.items():  \n",
    "    print(f\"Label {label}: {count} instances\")  \n",
    "  \n",
    "print(\"\\nVALIDATION SET:\")  \n",
    "for label, count in label_distribution_validation.items():  \n",
    "    print(f\"Label {label}: {count} instances\")  \n",
    "  \n",
    "print(\"\\nTEST SET:\")  \n",
    "for label, count in label_distribution_test.items():  \n",
    "    print(f\"Label {label}: {count} instances\")  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T11:17:25.175188500Z",
     "start_time": "2024-02-17T11:17:25.136260100Z"
    }
   },
   "id": "e62d5075680a1cd5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Save the training, validation, and test datasets to disk\n",
    "train.to_csv('../data/train.csv', index=False)\n",
    "validation.to_csv('../data/validation.csv', index=False)\n",
    "test.to_csv('../data/test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T11:23:31.959217300Z",
     "start_time": "2024-02-17T11:23:31.165346200Z"
    }
   },
   "id": "49f223ddca70706b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further processing if we need more stuff like processing for BERT models etc.."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d31752f89cb785c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "65a33e723e0a5608"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "deltapredictor",
   "language": "python",
   "display_name": "Python (dubby)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
